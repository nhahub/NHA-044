{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import mediapipe as mp\n",
        "import joblib\n",
        "from scipy.spatial import ConvexHull\n",
        "import zipfile\n",
        "import tempfile\n",
        "import shutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yBVuzoHdb2i",
        "outputId": "38332f04-4818-4ffe-8034-7cec9d6d745c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mp_hands = mp.solutions.hands\n",
        "train_folder_path = r'/content/train.zip'\n",
        "val_folder_path = r'/content/val_final.zip'\n",
        "test_folder_path = r'/content/test_final.zip'\n",
        "try:\n",
        "    basic_model_data = joblib.load('basic_enhanced_model.pkl')\n",
        "    label_to_idx = basic_model_data['label_to_idx']\n",
        "    idx_to_label = basic_model_data['idx_to_label']\n",
        "    class_names = basic_model_data['class_names']\n",
        "    print(\"Loaded class mappings from basic model\")\n",
        "except:\n",
        "    print(\"Basic model not found. Please run basic_enhanced_model.py first\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2YorH1ddfz5",
        "outputId": "dd3273b0-53ac-4ff4-ed69-0d9d112eb2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded class mappings from basic model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_zip(zip_path, extract_to):\n",
        "    \"\"\"\n",
        "    Extract zip file to temporary directory\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    return extract_to"
      ],
      "metadata": {
        "id": "rbKppFhaxWnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hand_landmarks(image_path, hands):\n",
        "    \"\"\"\n",
        "    Extract hand landmarks from image using MediaPipe\n",
        "    \"\"\"\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            return None\n",
        "\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        results = hands.process(image_rgb)\n",
        "\n",
        "        if results.multi_hand_landmarks:\n",
        "            hand_landmarks = results.multi_hand_landmarks[0]\n",
        "            landmarks = []\n",
        "            for landmark in hand_landmarks.landmark:\n",
        "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
        "\n",
        "            return np.array(landmarks)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "UwdK1d7wdre5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distance(point1, point2):\n",
        "    \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
        "    return np.sqrt((point1.x - point2.x)**2 +\n",
        "                  (point1.y - point2.y)**2 +\n",
        "                  (point1.z - point2.z)**2)"
      ],
      "metadata": {
        "id": "Zb7RbjOwd1WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_enhanced_features(hand_landmarks):\n",
        "    \"\"\"\n",
        "    Extract additional geometric features\n",
        "    \"\"\"\n",
        "    enhanced_features = []\n",
        "\n",
        "    landmarks = []\n",
        "    for i in range(0, len(hand_landmarks), 3):\n",
        "        class Point:\n",
        "            def __init__(self, x, y, z):\n",
        "                self.x, self.y, self.z = x, y, z\n",
        "        landmarks.append(Point(hand_landmarks[i], hand_landmarks[i+1], hand_landmarks[i+2]))\n",
        "\n",
        "    finger_tips = [4, 8, 12, 16, 20]\n",
        "    finger_mcps = [2, 5, 9, 13, 17]\n",
        "\n",
        "    for tip, mcp in zip(finger_tips, finger_mcps):\n",
        "        length = calculate_distance(landmarks[tip], landmarks[mcp])\n",
        "        enhanced_features.append(length)\n",
        "    palm_size = calculate_distance(landmarks[0], landmarks[9])\n",
        "    enhanced_features.append(palm_size)\n",
        "    return enhanced_features"
      ],
      "metadata": {
        "id": "zpWJOpkxd5nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_advanced_features(hand_landmarks):\n",
        "    \"\"\"\n",
        "    Extract even more sophisticated hand geometry features\n",
        "    \"\"\"\n",
        "    advanced_features = []\n",
        "    landmarks = []\n",
        "    for i in range(0, len(hand_landmarks), 3):\n",
        "        class Point:\n",
        "            def __init__(self, x, y, z):\n",
        "                self.x, self.y, self.z = x, y, z\n",
        "        landmarks.append(Point(hand_landmarks[i], hand_landmarks[i+1], hand_landmarks[i+2]))\n",
        "    palm_size = calculate_distance(landmarks[0], landmarks[9])\n",
        "    finger_tips = [4, 8, 12, 16, 20]\n",
        "    finger_mcps = [2, 5, 9, 13, 17]\n",
        "\n",
        "    for tip, mcp in zip(finger_tips, finger_mcps):\n",
        "        length = calculate_distance(landmarks[tip], landmarks[mcp])\n",
        "        relative_length = length / palm_size if palm_size > 0 else 0\n",
        "        advanced_features.append(relative_length)\n",
        "    for tip, pip, mcp in [(8, 6, 5), (12, 10, 9), (16, 14, 13), (20, 18, 17)]:\n",
        "        curvature = calculate_curvature(landmarks[tip], landmarks[pip], landmarks[mcp])\n",
        "        advanced_features.append(curvature)\n",
        "    convexity_features = calculate_convexity_features(landmarks)\n",
        "    advanced_features.extend(convexity_features)\n",
        "    inter_finger_dists = calculate_inter_finger_distances(landmarks)\n",
        "    advanced_features.extend(inter_finger_dists)\n",
        "    palm_center = calculate_palm_center(landmarks)\n",
        "    for tip_idx in finger_tips:\n",
        "        dist = calculate_distance(palm_center, landmarks[tip_idx])\n",
        "        advanced_features.append(dist)\n",
        "\n",
        "    return advanced_features"
      ],
      "metadata": {
        "id": "WViEPSmud-YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_curvature(tip, pip, mcp):\n",
        "    \"\"\"Calculate finger curvature\"\"\"\n",
        "    v1 = np.array([pip.x - mcp.x, pip.y - mcp.y])\n",
        "    v2 = np.array([tip.x - mcp.x, tip.y - mcp.y])\n",
        "\n",
        "    if np.linalg.norm(v1) > 0 and np.linalg.norm(v2) > 0:\n",
        "        cross_product = np.cross(v1, v2)\n",
        "        return cross_product / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "    return 0"
      ],
      "metadata": {
        "id": "UXQPSOhzyG8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_convexity_features(landmarks):\n",
        "    \"\"\"Calculate hand convexity and compactness\"\"\"\n",
        "    points = np.array([[lm.x, lm.y] for lm in landmarks])\n",
        "\n",
        "    try:\n",
        "        hull = ConvexHull(points)\n",
        "        hull_area = hull.volume\n",
        "        rect_area = (np.max(points[:,0]) - np.min(points[:,0])) * (np.max(points[:,1]) - np.min(points[:,1]))\n",
        "        convexity_ratio = hull_area / rect_area if rect_area > 0 else 0\n",
        "    except:\n",
        "        convexity_ratio = 0\n",
        "\n",
        "    return [convexity_ratio]"
      ],
      "metadata": {
        "id": "GIo8HezleB7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_inter_finger_distances(landmarks):\n",
        "    \"\"\"Calculate distances between finger tips\"\"\"\n",
        "    finger_tips = [4, 8, 12, 16, 20]\n",
        "    distances = []\n",
        "\n",
        "    for i in range(len(finger_tips)):\n",
        "        for j in range(i+1, len(finger_tips)):\n",
        "            dist = calculate_distance(landmarks[finger_tips[i]], landmarks[finger_tips[j]])\n",
        "            distances.append(dist)\n",
        "\n",
        "    return distances"
      ],
      "metadata": {
        "id": "Gccp7-dFeGZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_palm_center(landmarks):\n",
        "    \"\"\"Calculate approximate palm center\"\"\"\n",
        "    palm_points = [landmarks[0], landmarks[5], landmarks[9], landmarks[13], landmarks[17]]\n",
        "    avg_x = sum(lm.x for lm in palm_points) / len(palm_points)\n",
        "    avg_y = sum(lm.y for lm in palm_points) / len(palm_points)\n",
        "    avg_z = sum(lm.z for lm in palm_points) / len(palm_points)\n",
        "\n",
        "    class Point:\n",
        "        def __init__(self, x, y, z):\n",
        "            self.x, self.y, self.z = x, y, z\n",
        "\n",
        "    return Point(avg_x, avg_y, avg_z)"
      ],
      "metadata": {
        "id": "cBA_9H_OeKR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_advanced_dataset_from_zip(zip_path, expected_features=None):\n",
        "    \"\"\"\n",
        "    Create dataset with advanced features from zip file\n",
        "    \"\"\"\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    print(f\"Extracting {zip_path} to {temp_dir}\")\n",
        "\n",
        "    try:\n",
        "        extract_zip(zip_path, temp_dir)\n",
        "\n",
        "        hands = mp_hands.Hands(\n",
        "            static_image_mode=True,\n",
        "            max_num_hands=1,\n",
        "            min_detection_confidence=0.5\n",
        "        )\n",
        "\n",
        "        landmarks_list = []\n",
        "        labels_list = []\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "        for root, dirs, files in os.walk(temp_dir):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    parent_folder = os.path.basename(root)\n",
        "                    if parent_folder == os.path.basename(temp_dir):\n",
        "                        for item in os.listdir(temp_dir):\n",
        "                            item_path = os.path.join(temp_dir, item)\n",
        "                            if os.path.isdir(item_path) and item != os.path.basename(temp_dir):\n",
        "                                parent_folder = item\n",
        "                                break\n",
        "                    image = cv2.imread(full_path)\n",
        "                    if image is None:\n",
        "                        skipped_count += 1\n",
        "                        continue\n",
        "\n",
        "                    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    results = hands.process(image_rgb)\n",
        "\n",
        "                    if results.multi_hand_landmarks:\n",
        "                        hand_landmarks = results.multi_hand_landmarks[0]\n",
        "                        basic_landmarks = []\n",
        "                        for landmark in hand_landmarks.landmark:\n",
        "                            basic_landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
        "                        basic_landmarks_array = np.array(basic_landmarks)\n",
        "                        enhanced_features = extract_enhanced_features(basic_landmarks_array)\n",
        "                        advanced_features = extract_advanced_features(basic_landmarks_array)\n",
        "                        all_features = np.concatenate([basic_landmarks_array, enhanced_features, advanced_features])\n",
        "                        if expected_features is not None and len(all_features) != expected_features:\n",
        "                            skipped_count += 1\n",
        "                            continue\n",
        "                        landmarks_list.append(all_features)\n",
        "                        labels_list.append(parent_folder)\n",
        "                        processed_count += 1\n",
        "                    else:\n",
        "                        skipped_count += 1\n",
        "\n",
        "        hands.close()\n",
        "        print(f\"  Processed {processed_count} images, skipped {skipped_count} (no hands/different features) from {zip_path}\")\n",
        "        return np.array(landmarks_list), np.array(labels_list)\n",
        "\n",
        "    finally:\n",
        "        shutil.rmtree(temp_dir)\n"
      ],
      "metadata": {
        "id": "mqz0AM1JeR2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ensemble_model(X_train, y_train):\n",
        "    \"\"\"Create ensemble of multiple classifiers\"\"\"\n",
        "    from sklearn.svm import SVC\n",
        "    svm_model = SVC(\n",
        "        kernel='rbf',\n",
        "        C=10,\n",
        "        gamma='scale',\n",
        "        probability=True,\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    mlp_model = MLPClassifier(\n",
        "        hidden_layer_sizes=(128, 64),\n",
        "        activation='relu',\n",
        "        alpha=0.01,\n",
        "        learning_rate='adaptive',\n",
        "        max_iter=1000,\n",
        "        random_state=42\n",
        "    )\n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('svm', svm_model),\n",
        "            ('rf', rf_model),\n",
        "            ('mlp', mlp_model)\n",
        "        ],\n",
        "        voting='soft',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    return ensemble"
      ],
      "metadata": {
        "id": "ZE0KUqUAeWgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_best_features(X_train, y_train, X_val, X_test, k=100):\n",
        "    \"\"\"Select most important features\"\"\"\n",
        "    selector = SelectKBest(score_func=f_classif, k=min(k, X_train.shape[1]))\n",
        "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "    X_val_selected = np.array([])\n",
        "    X_test_selected = np.array([])\n",
        "\n",
        "    if len(X_val) > 0 and X_val.shape[1] == X_train.shape[1]:\n",
        "        X_val_selected = selector.transform(X_val)\n",
        "    elif len(X_val) > 0:\n",
        "        print(f\" Validation set has {X_val.shape[1]} features, expected {X_train.shape[1]}. Skipping validation.\")\n",
        "\n",
        "    if len(X_test) > 0 and X_test.shape[1] == X_train.shape[1]:\n",
        "        X_test_selected = selector.transform(X_test)\n",
        "    elif len(X_test) > 0:\n",
        "        print(f\" Test set has {X_test.shape[1]} features, expected {X_train.shape[1]}. Skipping test.\")\n",
        "\n",
        "    print(f\"Selected {X_train_selected.shape[1]} best features from {X_train.shape[1]} total\")\n",
        "    return X_train_selected, X_val_selected, X_test_selected, selector"
      ],
      "metadata": {
        "id": "yLd45tgBecLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_confusion(actual, predicted, class_names):\n",
        "    \"\"\"Analyze which classes are most confused\"\"\"\n",
        "    cm = confusion_matrix(actual, predicted)\n",
        "\n",
        "    confusion_pairs = []\n",
        "    for i in range(len(class_names)):\n",
        "        for j in range(len(class_names)):\n",
        "            if i != j and cm[i, j] > 0:\n",
        "                confusion_pairs.append((class_names[i], class_names[j], cm[i, j]))\n",
        "\n",
        "\n",
        "    confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    print(\"\\nMost confused class pairs:\")\n",
        "    for true_class, pred_class, count in confusion_pairs[:10]:\n",
        "        print(f\"{true_class} {pred_class}: {count} times\")"
      ],
      "metadata": {
        "id": "zq8bpuC1eh7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_weak_classes(X_data, y_data, class_names, weak_classes, augmentation_factor=2):\n",
        "    \"\"\"Augment data for weak-performing classes\"\"\"\n",
        "    X_augmented = list(X_data)\n",
        "    y_augmented = list(y_data)\n",
        "\n",
        "    for class_name in weak_classes:\n",
        "        if class_name not in label_to_idx:\n",
        "            continue\n",
        "\n",
        "        class_idx = label_to_idx[class_name]\n",
        "        class_mask = (y_data == class_idx)\n",
        "        class_samples = X_data[class_mask]\n",
        "\n",
        "        if len(class_samples) == 0:\n",
        "            continue\n",
        "        for _ in range(augmentation_factor):\n",
        "            for sample in class_samples:\n",
        "                noise = np.random.normal(0, 0.01, sample.shape)\n",
        "                augmented_sample = sample + noise\n",
        "                X_augmented.append(augmented_sample)\n",
        "                y_augmented.append(class_idx)\n",
        "\n",
        "    return np.array(X_augmented), np.array(y_augmented)"
      ],
      "metadata": {
        "id": "HsM4pN1YemX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*50)\n",
        "print(\"ADVANCED FEATURES AND ENSEMBLE MODEL\")\n",
        "print(\"=\"*50)\n",
        "for zip_path in [train_folder_path, val_folder_path, test_folder_path]:\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Warning: {zip_path} not found!\")\n",
        "    else:\n",
        "        print(f\"Found: {zip_path}\")\n",
        "print(\"Extracting advanced features from training set...\")\n",
        "X_train_advanced, y_train_advanced = create_advanced_dataset_from_zip(train_folder_path)\n",
        "\n",
        "if len(X_train_advanced) == 0:\n",
        "    raise ValueError(\"No hand landmarks were extracted from training data!\")\n",
        "\n",
        "expected_features = X_train_advanced.shape[1]\n",
        "print(f\"Expected feature dimension: {expected_features}\")\n",
        "print(\"Processing validation set...\")\n",
        "X_val_advanced, y_val_advanced = create_advanced_dataset_from_zip(val_folder_path, expected_features=expected_features)\n",
        "\n",
        "print(\"Processing test set...\")\n",
        "X_test_advanced, y_test_advanced = create_advanced_dataset_from_zip(test_folder_path, expected_features=expected_features)\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"Training: {X_train_advanced.shape[0]} samples\")\n",
        "print(f\"Validation: {X_val_advanced.shape[0] if len(X_val_advanced) > 0 else 0} samples\")\n",
        "print(f\"Test: {X_test_advanced.shape[0] if len(X_test_advanced) > 0 else 0} samples\")\n",
        "print(f\"Advanced feature dimension: {X_train_advanced.shape[1]}\")\n",
        "y_train_advanced_encoded = np.array([label_to_idx[label] for label in y_train_advanced])\n",
        "y_val_advanced_encoded = np.array([label_to_idx[label] for label in y_val_advanced]) if len(y_val_advanced) > 0 else np.array([])\n",
        "y_test_advanced_encoded = np.array([label_to_idx[label] for label in y_test_advanced]) if len(y_test_advanced) > 0 else np.array([])\n",
        "scaler_advanced = StandardScaler()\n",
        "X_train_scaled_advanced = scaler_advanced.fit_transform(X_train_advanced)\n",
        "if len(X_val_advanced) > 0:\n",
        "    X_val_scaled_advanced = scaler_advanced.transform(X_val_advanced)\n",
        "else:\n",
        "    X_val_scaled_advanced = np.array([])\n",
        "\n",
        "if len(X_test_advanced) > 0:\n",
        "    X_test_scaled_advanced = scaler_advanced.transform(X_test_advanced)\n",
        "else:\n",
        "    X_test_scaled_advanced = np.array([])\n",
        "print(\"Performing feature selection...\")\n",
        "X_train_selected, X_val_selected, X_test_selected, feature_selector = select_best_features(\n",
        "    X_train_scaled_advanced, y_train_advanced_encoded, X_val_scaled_advanced, X_test_scaled_advanced, k=80\n",
        ")\n",
        "\n",
        "print(\"Training ensemble model...\")\n",
        "ensemble_model = create_ensemble_model(X_train_selected, y_train_advanced_encoded)\n",
        "ensemble_model.fit(X_train_selected, y_train_advanced_encoded)\n",
        "\n",
        "if len(X_val_selected) > 0 and len(y_val_advanced_encoded) > 0:\n",
        "    y_val_pred_ensemble = ensemble_model.predict(X_val_selected)\n",
        "    val_accuracy_ensemble = accuracy_score(y_val_advanced_encoded, y_val_pred_ensemble)\n",
        "    print(f\"Ensemble Validation Accuracy: {val_accuracy_ensemble:.4f}\")\n",
        "\n",
        "if len(X_test_selected) > 0 and len(y_test_advanced_encoded) > 0:\n",
        "    y_test_pred_ensemble = ensemble_model.predict(X_test_selected)\n",
        "    test_accuracy_ensemble = accuracy_score(y_test_advanced_encoded, y_test_pred_ensemble)\n",
        "    print(f\"Ensemble Test Accuracy: {test_accuracy_ensemble:.4f}\")\n",
        "\n",
        "    print(\"\\nEnsemble Classification Report:\")\n",
        "    print(classification_report(y_test_advanced_encoded, y_test_pred_ensemble, target_names=class_names))\n",
        "\n",
        "    analyze_confusion(y_test_advanced_encoded, y_test_pred_ensemble, class_names)\n",
        "\n",
        "weak_classes = ['seen', 'kaaf', 'fa', 'taa', 'toot']\n",
        "print(f\"\\nAugmenting weak classes: {weak_classes}\")\n",
        "\n",
        "X_train_augmented, y_train_augmented = augment_weak_classes(\n",
        "    X_train_selected, y_train_advanced_encoded, class_names, weak_classes\n",
        ")\n",
        "\n",
        "print(f\"Training data after augmentation: {X_train_augmented.shape[0]} samples\")\n",
        "\n",
        "print(\"Training final model with augmented data...\")\n",
        "final_model = create_ensemble_model(X_train_augmented, y_train_augmented)\n",
        "final_model.fit(X_train_augmented, y_train_augmented)\n",
        "\n",
        "if len(X_val_selected) > 0 and len(y_val_advanced_encoded) > 0:\n",
        "    y_val_pred_final = final_model.predict(X_val_selected)\n",
        "    val_accuracy_final = accuracy_score(y_val_advanced_encoded, y_val_pred_final)\n",
        "    print(f\"Final Model Validation Accuracy: {val_accuracy_final:.4f}\")\n",
        "\n",
        "if len(X_test_selected) > 0 and len(y_test_advanced_encoded) > 0:\n",
        "    y_test_pred_final = final_model.predict(X_test_selected)\n",
        "    test_accuracy_final = accuracy_score(y_test_advanced_encoded, y_test_pred_final)\n",
        "    print(f\"Final Model Test Accuracy: {test_accuracy_final:.4f}\")\n",
        "\n",
        "final_model_data = {\n",
        "    'ensemble_model': final_model,\n",
        "    'scaler': scaler_advanced,\n",
        "    'feature_selector': feature_selector,\n",
        "    'label_to_idx': label_to_idx,\n",
        "    'idx_to_label': idx_to_label,\n",
        "    'class_names': class_names\n",
        "}\n",
        "\n",
        "joblib.dump(final_model_data, 'final_ensemble_hand_model.pkl')\n",
        "print(\"\\nFinal ensemble model saved as 'final_ensemble_hand_model.pkl'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ADVANCED MODEL TRAINING COMPLETED!\")\n",
        "print(\"=\"*50)\n",
        "if len(X_val_selected) > 0:\n",
        "    print(f\"Final Validation Accuracy: {val_accuracy_final:.4f}\")\n",
        "if len(X_test_selected) > 0:\n",
        "    print(f\"Final Test Accuracy: {test_accuracy_final:.4f}\")\n",
        "print(f\"Number of advanced features: {X_train_advanced.shape[1]}\")\n",
        "print(f\"Number of selected features: {X_train_selected.shape[1]}\")\n",
        "print(f\"Number of classes: {len(class_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtZtCUz_eqte",
        "outputId": "83d8190a-38a8-4c0c-dffd-184b49bc77ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "ADVANCED FEATURES AND ENSEMBLE MODEL\n",
            "==================================================\n",
            "Found: /content/train.zip\n",
            "Found: /content/val_final.zip\n",
            "Found: /content/test_final.zip\n",
            "Extracting advanced features from training set...\n",
            "Extracting /content/train.zip to /tmp/tmpzfk6rti2\n",
            "  Processed 13606 images, skipped 596 (no hands/different features) from /content/train.zip\n",
            "Expected feature dimension: 94\n",
            "Processing validation set...\n",
            "Extracting /content/val_final.zip to /tmp/tmpk8qm21b6\n",
            "  Processed 2051 images, skipped 72 (no hands/different features) from /content/val_final.zip\n",
            "Processing test set...\n",
            "Extracting /content/test_final.zip to /tmp/tmpwr2mc0rr\n",
            "  Processed 2059 images, skipped 65 (no hands/different features) from /content/test_final.zip\n",
            "\n",
            "Dataset sizes:\n",
            "Training: 13606 samples\n",
            "Validation: 2051 samples\n",
            "Test: 2059 samples\n",
            "Advanced feature dimension: 94\n",
            "Performing feature selection...\n",
            "Selected 80 best features from 94 total\n",
            "Training ensemble model...\n",
            "Ensemble Validation Accuracy: 0.9966\n",
            "Ensemble Test Accuracy: 0.9976\n",
            "\n",
            "Ensemble Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ain       1.00      1.00      1.00        62\n",
            "          al       1.00      1.00      1.00        65\n",
            "       aleff       1.00      1.00      1.00        67\n",
            "          bb       1.00      1.00      1.00        66\n",
            "         dal       1.00      1.00      1.00        56\n",
            "         dha       1.00      1.00      1.00        67\n",
            "        dhad       1.00      0.97      0.98        65\n",
            "          fa       1.00      0.99      0.99        67\n",
            "        gaaf       0.98      1.00      0.99        62\n",
            "       ghain       1.00      1.00      1.00        67\n",
            "          ha       1.00      1.00      1.00        62\n",
            "         haa       1.00      1.00      1.00        57\n",
            "        jeem       1.00      1.00      1.00        63\n",
            "        kaaf       0.99      1.00      0.99        67\n",
            "        khaa       1.00      1.00      1.00        64\n",
            "          la       1.00      1.00      1.00        65\n",
            "        laam       1.00      1.00      1.00        65\n",
            "        meem       1.00      1.00      1.00        65\n",
            "         nun       1.00      1.00      1.00        61\n",
            "          ra       1.00      1.00      1.00        61\n",
            "        saad       1.00      1.00      1.00        66\n",
            "        seen       1.00      0.99      0.99        68\n",
            "       sheen       1.00      1.00      1.00        67\n",
            "          ta       0.98      1.00      0.99        65\n",
            "         taa       1.00      1.00      1.00        68\n",
            "        thaa       1.00      0.99      0.99        67\n",
            "        thal       1.00      1.00      1.00        65\n",
            "        toot       1.00      1.00      1.00        63\n",
            "         waw       0.97      1.00      0.98        60\n",
            "          ya       1.00      1.00      1.00        66\n",
            "         yaa       1.00      1.00      1.00        67\n",
            "         zay       1.00      1.00      1.00        63\n",
            "\n",
            "    accuracy                           1.00      2059\n",
            "   macro avg       1.00      1.00      1.00      2059\n",
            "weighted avg       1.00      1.00      1.00      2059\n",
            "\n",
            "\n",
            "Most confused class pairs:\n",
            "dhad waw: 2 times\n",
            "fa gaaf: 1 times\n",
            "seen kaaf: 1 times\n",
            "thaa ta: 1 times\n",
            "\n",
            "Augmenting weak classes: ['seen', 'kaaf', 'fa', 'taa', 'toot']\n",
            "Training data after augmentation: 17960 samples\n",
            "Training final model with augmented data...\n",
            "Final Model Validation Accuracy: 0.9980\n",
            "Final Model Test Accuracy: 0.9985\n",
            "\n",
            "Final ensemble model saved as 'final_ensemble_hand_model.pkl'\n",
            "\n",
            "==================================================\n",
            "ADVANCED MODEL TRAINING COMPLETED!\n",
            "==================================================\n",
            "Final Validation Accuracy: 0.9980\n",
            "Final Test Accuracy: 0.9985\n",
            "Number of advanced features: 94\n",
            "Number of selected features: 80\n",
            "Number of classes: 32\n"
          ]
        }
      ]
    }
  ]
}